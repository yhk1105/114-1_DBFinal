#!/usr/bin/env python3
"""
æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦è…³æœ¬

æ¸¬è©¦ 3 å€‹æœ€è¤‡é›œæŸ¥è©¢çš„åŸ·è¡Œæ™‚é–“ï¼Œä¸¦æ¯”è¼ƒä¸åŒç´¢å¼•é…ç½®çš„æ•ˆèƒ½å·®ç•°ã€‚
"""

import psycopg2
import time
import statistics
import argparse
import json
import os
from datetime import datetime, timedelta
from typing import List, Dict

# è³‡æ–™åº«é€£ç·šè¨­å®š
DB_CONFIG = {
    'host': os.getenv('DB_HOST', 'localhost'),
    'port': os.getenv('DB_PORT', 5432),
    'database': os.getenv('DB_NAME', 'your_database'),
    'user': os.getenv('DB_USER', 'your_user'),
    'password': os.getenv('DB_PASSWORD', 'your_password')
}


def get_connection():
    """å»ºç«‹è³‡æ–™åº«é€£ç·š"""
    return psycopg2.connect(**DB_CONFIG)


def clear_cache(conn):
    """æ¸…ç©ºè³‡æ–™åº« cache"""
    cursor = conn.cursor()
    cursor.execute("DISCARD ALL;")
    conn.commit()


def benchmark_time_conflict_check(conn, iterations=100) -> List[float]:
    """
    æ¸¬è©¦æŸ¥è©¢ 1ï¼šæ™‚é–“è¡çªæª¢æŸ¥ï¼ˆOVERLAPSï¼‰

    é€™æ˜¯ç³»çµ±ä¸­æœ€è¤‡é›œçš„æŸ¥è©¢ä¹‹ä¸€ï¼Œæ¯æ¬¡é ç´„éƒ½æœƒåŸ·è¡Œã€‚
    """
    cursor = conn.cursor()
    times = []

    # ç²å–éš¨æ©Ÿçš„ item_id
    cursor.execute("SELECT i_id FROM item WHERE status = 'Reservable' ORDER BY RANDOM() LIMIT 100;")
    item_ids = [row[0] for row in cursor.fetchall()]

    print(f"  æ¸¬è©¦æ™‚é–“è¡çªæª¢æŸ¥æŸ¥è©¢ï¼ˆ{iterations} æ¬¡ï¼‰...")

    for i in range(iterations):
        item_id = item_ids[i % len(item_ids)]
        start_time = datetime.now()
        end_time = start_time + timedelta(days=7)

        # æ¸…ç©º cache
        if i % 10 == 0:
            clear_cache(conn)

        query_start = time.time()
        cursor.execute("""
            SELECT rd.rd_id
            FROM reservation_detail rd
            JOIN reservation r ON rd.r_id = r.r_id
            WHERE rd.i_id = %s
            AND r.is_deleted = false
            AND ((rd.est_start_at, rd.est_due_at) OVERLAPS (%s, %s))
        """, (item_id, start_time, end_time))

        result = cursor.fetchall()
        query_time = (time.time() - query_start) * 1000  # è½‰æ›ç‚ºæ¯«ç§’

        times.append(query_time)

        if (i + 1) % 20 == 0:
            print(f"    é€²åº¦ï¼š{i + 1}/{iterations}")

    return times


def benchmark_contribution_check(conn, iterations=100) -> List[float]:
    """
    æ¸¬è©¦æŸ¥è©¢ 2ï¼šè²¢ç»æª¢æŸ¥èˆ‡éè¿´åˆ†é¡æŸ¥è©¢

    åŒ…å« WITH RECURSIVE å’Œå¤šå±¤ JOINï¼Œæ˜¯ç³»çµ±ä¸­æœ€è¤‡é›œçš„æŸ¥è©¢ã€‚
    """
    cursor = conn.cursor()
    times = []

    # ç²å–éš¨æ©Ÿçš„ç”¨æˆ¶å’Œé¡åˆ¥
    cursor.execute("""
        SELECT DISTINCT m.m_id, c.c_id
        FROM member m
        CROSS JOIN category c
        WHERE c.parent_c_id IS NULL
        ORDER BY RANDOM()
        LIMIT 100;
    """)
    test_data = cursor.fetchall()

    print(f"  æ¸¬è©¦è²¢ç»æª¢æŸ¥æŸ¥è©¢ï¼ˆ{iterations} æ¬¡ï¼‰...")

    for i in range(iterations):
        m_id, root_c_id = test_data[i % len(test_data)]

        # æ¸…ç©º cache
        if i % 10 == 0:
            clear_cache(conn)

        query_start = time.time()
        cursor.execute("""
            SELECT contribution.i_id
            FROM contribution
            JOIN item ON contribution.i_id = item.i_id
            WHERE contribution.m_id = %s
            AND contribution.is_active = true
            AND item.c_id IN (
                WITH RECURSIVE category_tree AS (
                    SELECT c_id FROM category WHERE c_id = %s
                    UNION ALL
                    SELECT c.c_id FROM category c
                    JOIN category_tree ct ON c.parent_c_id = ct.c_id
                )
                SELECT c_id FROM category_tree
            )
            LIMIT 1
        """, (m_id, root_c_id))

        result = cursor.fetchall()
        query_time = (time.time() - query_start) * 1000

        times.append(query_time)

        if (i + 1) % 20 == 0:
            print(f"    é€²åº¦ï¼š{i + 1}/{iterations}")

    return times


def benchmark_rating_calculation(conn, iterations=100) -> List[float]:
    """
    æ¸¬è©¦æŸ¥è©¢ 3ï¼šç”¨æˆ¶è©•åˆ†è¨ˆç®—

    åŒ…å« CTEã€å¤šå±¤ JOIN å’Œ AVG() èšåˆå‡½æ•¸ã€‚
    """
    cursor = conn.cursor()
    times = []

    # ç²å–éš¨æ©Ÿç”¨æˆ¶
    cursor.execute("SELECT m_id FROM member ORDER BY RANDOM() LIMIT 100;")
    member_ids = [row[0] for row in cursor.fetchall()]

    print(f"  æ¸¬è©¦è©•åˆ†è¨ˆç®—æŸ¥è©¢ï¼ˆ{iterations} æ¬¡ï¼‰...")

    for i in range(iterations):
        m_id = member_ids[i % len(member_ids)]

        # æ¸…ç©º cache
        if i % 10 == 0:
            clear_cache(conn)

        query_start = time.time()
        cursor.execute("""
            WITH owner_rate AS (
                SELECT i.m_id, AVG(rv.score) as owner_rate
                FROM review rv
                JOIN loan l ON rv.l_id = l.l_id
                JOIN reservation_detail rd ON l.rd_id = rd.rd_id
                JOIN item i ON rd.i_id = i.i_id
                WHERE rv.reviewee_id = %s AND i.m_id = %s AND rv.is_deleted = false
                GROUP BY i.m_id
            ),
            borrower_rate AS (
                SELECT r.m_id, AVG(rv.score) as borrower_rate
                FROM review rv
                JOIN loan l ON rv.l_id = l.l_id
                JOIN reservation_detail rd ON l.rd_id = rd.rd_id
                JOIN reservation r ON rd.r_id = r.r_id
                WHERE rv.reviewee_id = %s AND r.m_id = %s AND rv.is_deleted = false
                GROUP BY r.m_id
            )
            SELECT m.m_name, m.m_mail,
                   owner_rate.owner_rate,
                   borrower_rate.borrower_rate
            FROM member m
            LEFT JOIN owner_rate ON m.m_id = owner_rate.m_id
            LEFT JOIN borrower_rate ON m.m_id = borrower_rate.m_id
            WHERE m.m_id = %s
        """, (m_id, m_id, m_id, m_id, m_id))

        result = cursor.fetchall()
        query_time = (time.time() - query_start) * 1000

        times.append(query_time)

        if (i + 1) % 20 == 0:
            print(f"    é€²åº¦ï¼š{i + 1}/{iterations}")

    return times


def analyze_results(times: List[float], query_name: str) -> Dict:
    """åˆ†ææ¸¬è©¦çµæœ"""
    times_sorted = sorted(times)
    p95_index = int(len(times_sorted) * 0.95)
    p99_index = int(len(times_sorted) * 0.99)

    results = {
        'query_name': query_name,
        'iterations': len(times),
        'avg_ms': statistics.mean(times),
        'median_ms': statistics.median(times),
        'min_ms': min(times),
        'max_ms': max(times),
        'p95_ms': times_sorted[p95_index],
        'p99_ms': times_sorted[p99_index],
        'stddev_ms': statistics.stdev(times) if len(times) > 1 else 0,
        'qps': 1000 / statistics.mean(times) if statistics.mean(times) > 0 else 0
    }

    return results


def print_results(results: Dict):
    """å°å‡ºæ¸¬è©¦çµæœ"""
    print(f"\n  ğŸ“Š {results['query_name']} æ¸¬è©¦çµæœ:")
    print(f"     å¹³å‡åŸ·è¡Œæ™‚é–“: {results['avg_ms']:.2f} ms")
    print(f"     ä¸­ä½æ•¸: {results['median_ms']:.2f} ms")
    print(f"     æœ€å°å€¼: {results['min_ms']:.2f} ms")
    print(f"     æœ€å¤§å€¼: {results['max_ms']:.2f} ms")
    print(f"     P95: {results['p95_ms']:.2f} ms")
    print(f"     P99: {results['p99_ms']:.2f} ms")
    print(f"     æ¨™æº–å·®: {results['stddev_ms']:.2f} ms")
    print(f"     ååé‡: {results['qps']:.2f} QPS")


def main():
    parser = argparse.ArgumentParser(description='è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦')
    parser.add_argument('--iterations', type=int, default=100, help='æ¸¬è©¦æ¬¡æ•¸ï¼ˆé è¨­ï¼š100ï¼‰')
    parser.add_argument('--query', choices=['all', 'time_conflict', 'contribution', 'rating'],
                        default='all', help='æ¸¬è©¦çš„æŸ¥è©¢é¡å‹')
    parser.add_argument('--output', type=str, help='çµæœè¼¸å‡ºæª”æ¡ˆï¼ˆJSON æ ¼å¼ï¼‰')

    args = parser.parse_args()

    print("=" * 70)
    print("ğŸ” è³‡æ–™åº«æŸ¥è©¢æ•ˆèƒ½æ¸¬è©¦")
    print("=" * 70)
    print(f"\næ¸¬è©¦æ¬¡æ•¸ï¼š{args.iterations}")
    print(f"æ¸¬è©¦æŸ¥è©¢ï¼š{args.query}\n")

    conn = get_connection()
    all_results = []

    try:
        if args.query in ['all', 'time_conflict']:
            print("\n1ï¸âƒ£  æ¸¬è©¦æŸ¥è©¢ 1ï¼šæ™‚é–“è¡çªæª¢æŸ¥ï¼ˆOVERLAPSï¼‰")
            times = benchmark_time_conflict_check(conn, args.iterations)
            results = analyze_results(times, "æ™‚é–“è¡çªæª¢æŸ¥")
            print_results(results)
            all_results.append(results)

        if args.query in ['all', 'contribution']:
            print("\n2ï¸âƒ£  æ¸¬è©¦æŸ¥è©¢ 2ï¼šè²¢ç»æª¢æŸ¥èˆ‡éè¿´åˆ†é¡")
            times = benchmark_contribution_check(conn, args.iterations)
            results = analyze_results(times, "è²¢ç»æª¢æŸ¥")
            print_results(results)
            all_results.append(results)

        if args.query in ['all', 'rating']:
            print("\n3ï¸âƒ£  æ¸¬è©¦æŸ¥è©¢ 3ï¼šç”¨æˆ¶è©•åˆ†è¨ˆç®—")
            times = benchmark_rating_calculation(conn, args.iterations)
            results = analyze_results(times, "è©•åˆ†è¨ˆç®—")
            print_results(results)
            all_results.append(results)

        # è¼¸å‡ºçµæœåˆ°æª”æ¡ˆ
        if args.output:
            with open(args.output, 'w', encoding='utf-8') as f:
                json.dump({
                    'timestamp': datetime.now().isoformat(),
                    'iterations': args.iterations,
                    'results': all_results
                }, f, indent=2, ensure_ascii=False)
            print(f"\nğŸ’¾ çµæœå·²å„²å­˜è‡³ï¼š{args.output}")

        print("\n" + "=" * 70)
        print("âœ… æ¸¬è©¦å®Œæˆï¼")
        print("=" * 70)

    except Exception as e:
        print(f"\nâŒ éŒ¯èª¤ï¼š{e}")
        import traceback
        traceback.print_exc()
    finally:
        conn.close()


if __name__ == '__main__':
    main()
