#!/usr/bin/env python3
"""
PostgreSQL CSV åŒ¯å…¥å·¥å…·
ä½¿ç”¨ pandas å’Œ psycopg2 å°‡ CSV æª”æ¡ˆåŒ¯å…¥ PostgreSQL è³‡æ–™åº«
åŒ…å«å®Œæ•´çš„è³‡æ–™åº«åˆå§‹åŒ–æµç¨‹
"""

import os
import csv
import psycopg2
from psycopg2.extras import execute_values
from urllib.parse import urlparse
from dotenv import load_dotenv

# è¼‰å…¥ç’°å¢ƒè®Šæ•¸
load_dotenv()

# è³‡æ–™åº«é€£ç·šè¨­å®š
DATABASE_URL = os.getenv("DATABASE_URL")
TARGET_DB_NAME = "our_things"

# CSV æª”æ¡ˆè·¯å¾‘ï¼ˆç›¸å°æ–¼æ­¤è…³æœ¬ï¼‰
# è…³æœ¬åœ¨ backend/app/db/import_csv.pyï¼ŒCSV åœ¨ backend/app/db/csv/
CSV_DIR = "csv"

# SQL æª”æ¡ˆè·¯å¾‘ï¼ˆç›¸å°æ–¼æ­¤è…³æœ¬ï¼‰
# è…³æœ¬åœ¨ backend/app/db/SetDB.pyï¼ŒSQL åœ¨ backend/app/db/
SCHEMA_SQL_PATH = "schema.sql"
SETNEXTVAL_SQL_PATH = "setnextval.sql"
SETINDEX_SQL_PATH = "setindex.sql"

# è¡¨æ ¼èˆ‡ CSV æª”æ¡ˆçš„å°æ‡‰é—œä¿‚
TABLE_MAPPINGS = {
    "member": {
        "file": "member.csv",
        "columns": ["m_id", "m_name", "m_mail", "m_password", "is_active"]
    },
    "category": {
        "file": "category.csv",
        "columns": ["c_id", "c_name", "parent_c_id"]
    },
    "staff": {
        "file": "staff.csv",
        "columns": ["s_id", "s_name", "s_mail", "s_password", "role", "is_deleted"]
    },
    "pick_up_place": {
        "file": "pickup_place.csv",
        "columns": ["p_id", "p_name", "address", "note", "is_deleted"]
    },
    "item": {
        "file": "item.csv",
        "columns": ["i_id", "i_name", "status", "description", "out_duration", "m_id", "c_id"]
    },
    "item_pick": {
        "file": "item_pick.csv",
        "columns": ["i_id", "p_id", "is_deleted"]
    },
    "item_verification": {
        "file": "item_verification.csv",
        "columns": ["iv_id", "v_conclusion", "create_at", "i_id", "s_id"]
    },
    "reservation": {
        "file": "reservation.csv",
        "columns": ["r_id", "is_deleted", "create_at", "m_id"]
    },
    "reservation_detail": {
        "file": "reservation_detail.csv",
        "columns": ["rd_id", "est_start_at", "est_due_at", "r_id", "i_id", "p_id"]
    },
    "contribution": {
        "file": "contribution.csv",
        "columns": ["m_id", "i_id", "is_active"]
    },
    "category_ban": {
        "file": "category_ban.csv",
        "columns": ["s_id", "c_id", "m_id", "is_deleted", "ban_at"]
    },
    "report": {
        "file": "report.csv",
        "columns": ["re_id", "comment", "r_conclusion", "create_at", "conclude_at", "m_id", "i_id", "s_id"]
    },
    "loan": {
        "file": "loan.csv",
        "columns": ["l_id", "rd_id", "actual_start_at", "actual_return_at", "is_deleted"]
    },
    "loan_event": {
        "file": "loan_event.csv",
        "columns": ["timestamp", "event_type", "l_id"]
    },
    "review": {
        "file": "review.csv",
        "columns": ["review_id", "score", "comment", "reviewer_id", "reviewee_id", "l_id", "is_deleted"]
    }
}


def parse_database_url(database_url):
    """è§£æ DATABASE_URL ä¸¦è¿”å›é€£ç·šåƒæ•¸"""
    parsed = urlparse(database_url)
    return {
        'host': parsed.hostname or 'localhost',
        'port': parsed.port or 5432,
        'user': parsed.username,
        'password': parsed.password,
        'database': parsed.path.lstrip('/') if parsed.path else 'postgres'
    }


def get_admin_connection_params(database_url):
    """å–å¾—ç®¡ç†å“¡é€£ç·šåƒæ•¸ï¼ˆé€£æ¥åˆ° postgres è³‡æ–™åº«ï¼‰"""
    params = parse_database_url(database_url)
    params['database'] = 'postgres'  # é€£æ¥åˆ° postgres è³‡æ–™åº«ä¾†åŸ·è¡Œç®¡ç†æ“ä½œ
    return params


def check_database_exists(conn_params, db_name):
    """æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨"""
    try:
        conn = psycopg2.connect(**conn_params)
        conn.autocommit = True
        cursor = conn.cursor()
        cursor.execute(
            "SELECT 1 FROM pg_database WHERE datname = %s",
            (db_name,)
        )
        exists = cursor.fetchone() is not None
        cursor.close()
        conn.close()
        return exists
    except (psycopg2.Error, IOError) as e:
        print(f"   âš ï¸  æª¢æŸ¥è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False


def drop_database(conn_params, db_name):
    """å¼·åˆ¶åˆªé™¤è³‡æ–™åº«"""
    try:
        conn = psycopg2.connect(**conn_params)
        conn.autocommit = True
        cursor = conn.cursor()

        # çµ‚æ­¢æ‰€æœ‰é€£æ¥åˆ°è©²è³‡æ–™åº«çš„é€£ç·š
        cursor.execute("""
            SELECT pg_terminate_backend(pg_stat_activity.pid)
            FROM pg_stat_activity
            WHERE pg_stat_activity.datname = %s
            AND pid <> pg_backend_pid();
        """, (db_name,))

        # å¼·åˆ¶åˆªé™¤è³‡æ–™åº«ï¼ˆPostgreSQL 13+ æ”¯æ´ WITH (FORCE)ï¼‰
        try:
            cursor.execute(
                f'DROP DATABASE IF EXISTS "{db_name}" WITH (FORCE);')
        except psycopg2.Error:
            # å¦‚æœä¸æ”¯æ´ WITH (FORCE)ï¼Œä½¿ç”¨å‚³çµ±æ–¹å¼
            cursor.execute(f'DROP DATABASE IF EXISTS "{db_name}";')

        cursor.close()
        conn.close()
        print(f"   âœ… å·²åˆªé™¤è³‡æ–™åº«: {db_name}")
        return True
    except (psycopg2.Error, IOError) as e:
        print(f"   âš ï¸  åˆªé™¤è³‡æ–™åº«æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return False


def create_database(conn_params, db_name):
    """å»ºç«‹è³‡æ–™åº«"""
    try:
        conn = psycopg2.connect(**conn_params)
        conn.autocommit = True
        cursor = conn.cursor()
        cursor.execute(f'CREATE DATABASE "{db_name}";')
        cursor.close()
        conn.close()
        print(f"   âœ… å·²å»ºç«‹è³‡æ–™åº«: {db_name}")
        return True
    except (psycopg2.Error, IOError) as e:
        print(f"   âŒ å»ºç«‹è³‡æ–™åº«å¤±æ•—: {str(e)}")
        return False


def execute_sql_file(conn, sql_file_path):
    """åŸ·è¡Œ SQL æª”æ¡ˆ"""
    try:
        # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„çµ•å°è·¯å¾‘
        script_dir = os.path.dirname(os.path.abspath(__file__))
        # æ§‹å»º SQL æª”æ¡ˆçš„çµ•å°è·¯å¾‘
        abs_sql_path = os.path.abspath(os.path.join(script_dir, sql_file_path))

        if not os.path.exists(abs_sql_path):
            print(f"   âš ï¸  SQL æª”æ¡ˆä¸å­˜åœ¨: {abs_sql_path}")
            return False

        print(f"   ğŸ“„ åŸ·è¡Œ SQL æª”æ¡ˆ: {os.path.basename(sql_file_path)}")

        with open(abs_sql_path, 'r', encoding='utf-8') as f:
            sql_content = f.read()

        cursor = conn.cursor()

        # ä½¿ç”¨ psycopg2 çš„ execute() åŸ·è¡Œ SQL
        # PostgreSQL å…è¨±åœ¨ä¸€å€‹ execute() ä¸­åŸ·è¡Œå¤šå€‹èªå¥ï¼ˆç”¨åˆ†è™Ÿåˆ†éš”ï¼‰
        # ä½†ç‚ºäº†æ›´å¥½çš„éŒ¯èª¤è™•ç†ï¼Œæˆ‘å€‘é€è¡Œè™•ç†ä¸¦æŒ‰åˆ†è™Ÿåˆ†å‰²
        statements = []
        current_stmt = ""

        for line in sql_content.split('\n'):
            stripped = line.strip()
            # è·³éç©ºè¡Œå’Œè¨»è§£è¡Œ
            if not stripped or stripped.startswith('--'):
                continue

            current_stmt += line + '\n'

            # å¦‚æœé€™ä¸€è¡Œä»¥åˆ†è™Ÿçµå°¾ï¼Œè¡¨ç¤ºä¸€å€‹å®Œæ•´çš„èªå¥
            if stripped.endswith(';'):
                stmt = current_stmt.strip()
                if stmt:
                    statements.append(stmt)
                current_stmt = ""

        # è™•ç†æœ€å¾Œä¸€å€‹èªå¥ï¼ˆå¦‚æœæ²’æœ‰ä»¥åˆ†è™Ÿçµå°¾ï¼‰
        if current_stmt.strip():
            statements.append(current_stmt.strip())

        # åŸ·è¡Œæ¯å€‹èªå¥
        executed_count = 0
        for stmt in statements:
            if stmt:
                try:
                    cursor.execute(stmt)
                    executed_count += 1
                except psycopg2.Error as e:
                    # æŸäº›éŒ¯èª¤å¯ä»¥å¿½ç•¥ï¼ˆå¦‚å·²å­˜åœ¨çš„ç‰©ä»¶ï¼‰
                    error_msg = str(e).lower()
                    if 'already exists' not in error_msg and 'does not exist' not in error_msg:
                        print(f"      âš ï¸  åŸ·è¡Œèªå¥æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)[:100]}")
                        # ç¹¼çºŒåŸ·è¡Œå…¶ä»–èªå¥

        conn.commit()
        cursor.close()

        print(f"   âœ… SQL æª”æ¡ˆåŸ·è¡ŒæˆåŠŸï¼ˆåŸ·è¡Œ {executed_count} å€‹èªå¥ï¼‰")
        return True
    except (psycopg2.Error, IOError, ValueError) as e:
        conn.rollback()
        print(f"   âŒ åŸ·è¡Œ SQL æª”æ¡ˆå¤±æ•—: {str(e)}")
        import traceback
        traceback.print_exc()
        return False


def read_csv(file_path):
    """è®€å– CSV æª”æ¡ˆä¸¦è¿”å›è³‡æ–™åˆ—è¡¨"""
    data = []
    with open(file_path, 'r', encoding='utf-8-sig') as f:  # ä½¿ç”¨ utf-8-sig è‡ªå‹•å»é™¤ BOM
        reader = csv.DictReader(f)
        # å–å¾—å¯¦éš›çš„æ¬„ä½åç¨±ï¼ˆå»é™¤å‰å¾Œç©ºæ ¼å’Œ BOMï¼‰
        fieldnames = [name.strip().lstrip('\ufeff')
                      for name in reader.fieldnames] if reader.fieldnames else []
        reader.fieldnames = fieldnames

        for row in reader:
            # å°‡ç©ºå­—ä¸²è½‰æ›ç‚º Noneï¼ˆNULLï¼‰
            processed_row = {}
            for key, value in row.items():
                # å»é™¤éµçš„å‰å¾Œç©ºæ ¼å’Œ BOM
                clean_key = key.strip().lstrip('\ufeff') if key else key
                if value == '' or value is None:
                    processed_row[clean_key] = None
                else:
                    processed_row[clean_key] = value.strip(
                    ) if isinstance(value, str) else value
            data.append(processed_row)
    return data


def import_table(conn, table_name, mapping):
    """åŒ¯å…¥å–®ä¸€è¡¨æ ¼"""
    # å–å¾—è…³æœ¬æ‰€åœ¨ç›®éŒ„çš„çµ•å°è·¯å¾‘
    script_dir = os.path.dirname(os.path.abspath(__file__))
    # æ§‹å»º CSV æª”æ¡ˆçš„çµ•å°è·¯å¾‘
    file_path = os.path.abspath(os.path.join(
        script_dir, CSV_DIR, mapping["file"]))

    if not os.path.exists(file_path):
        print(f"âš ï¸  æª”æ¡ˆä¸å­˜åœ¨: {file_path}")
        return False

    print(f"ğŸ“‚ æ­£åœ¨åŒ¯å…¥ {table_name}...")

    try:
        # è®€å– CSV
        data = read_csv(file_path)

        if not data:
            print(f"   âš ï¸  {table_name} æª”æ¡ˆç‚ºç©º")
            return False

        # æº–å‚™è³‡æ–™
        columns = mapping["columns"]
        values = []

        # æª¢æŸ¥ç¬¬ä¸€ç­†è³‡æ–™æ˜¯å¦åŒ…å«æ‰€æœ‰éœ€è¦çš„æ¬„ä½
        if data:
            missing_cols = [col for col in columns if col not in data[0]]
            if missing_cols:
                print(f"   âŒ CSV æª”æ¡ˆç¼ºå°‘æ¬„ä½: {missing_cols}")
                print(f"   â„¹ï¸  CSV æª”æ¡ˆå¯¦éš›æ¬„ä½: {list(data[0].keys())}")
                return False

        for row in data:
            # åªå–éœ€è¦çš„æ¬„ä½ï¼Œä¸¦æŒ‰ç…§é †åºæ’åˆ—
            row_values = []
            for col in columns:
                value = row.get(col)
                if value is None and col in row:
                    # æ¬„ä½å­˜åœ¨ä½†å€¼ç‚ºç©ºå­—ä¸²ï¼ˆå·²åœ¨ read_csv ä¸­è½‰ç‚º Noneï¼‰
                    pass
                elif value is None:
                    # æ¬„ä½ä¸å­˜åœ¨
                    print(f"   âš ï¸  è­¦å‘Š: æ¬„ä½ '{col}' ä¸å­˜åœ¨æ–¼ CSV ä¸­ï¼Œå°‡è¨­ç‚º NULL")
                row_values.append(value)
            values.append(row_values)

        # å»ºç«‹ SQL èªå¥
        # execute_values éœ€è¦çš„æ ¼å¼ï¼šINSERT INTO table (cols) VALUES %s
        columns_str = ','.join(columns)
        sql = f"INSERT INTO {table_name} ({columns_str}) VALUES %s"

        # åŸ·è¡Œæ’å…¥
        cursor = conn.cursor()
        execute_values(cursor, sql, values, page_size=1000)
        conn.commit()
        cursor.close()

        print(f"   âœ… æˆåŠŸåŒ¯å…¥ {len(values)} ç­†è³‡æ–™åˆ° {table_name}")
        return True

    except (psycopg2.Error, IOError, ValueError) as e:
        conn.rollback()
        print(f"   âŒ åŒ¯å…¥ {table_name} å¤±æ•—: {str(e)}")
        return False


def main():
    """ä¸»å‡½æ•¸"""
    if not DATABASE_URL:
        print("âŒ éŒ¯èª¤: è«‹è¨­å®š DATABASE_URL ç’°å¢ƒè®Šæ•¸")
        return

    print("ğŸš€ é–‹å§‹åˆå§‹åŒ–è³‡æ–™åº«ä¸¦åŒ¯å…¥ CSV è³‡æ–™...")
    print(f"ğŸ“ CSV ç›®éŒ„: {CSV_DIR}")
    print(f"ğŸ¯ ç›®æ¨™è³‡æ–™åº«: {TARGET_DB_NAME}\n")

    try:
        # æ­¥é©Ÿ 1: å–å¾—ç®¡ç†å“¡é€£ç·šåƒæ•¸
        admin_params = get_admin_connection_params(DATABASE_URL)

        # æ­¥é©Ÿ 2: æª¢æŸ¥ä¸¦åˆªé™¤ç¾æœ‰è³‡æ–™åº«
        print("ğŸ“‹ æ­¥é©Ÿ 1: æª¢æŸ¥è³‡æ–™åº«æ˜¯å¦å­˜åœ¨...")
        if check_database_exists(admin_params, TARGET_DB_NAME):
            print(f"   âš ï¸  è³‡æ–™åº« {TARGET_DB_NAME} å·²å­˜åœ¨")
            print("ğŸ“‹ æ­¥é©Ÿ 2: å¼·åˆ¶åˆªé™¤ç¾æœ‰è³‡æ–™åº«...")
            if not drop_database(admin_params, TARGET_DB_NAME):
                print("âŒ ç„¡æ³•åˆªé™¤è³‡æ–™åº«ï¼Œçµ‚æ­¢ç¨‹åº")
                return
        else:
            print(f"   â„¹ï¸  è³‡æ–™åº« {TARGET_DB_NAME} ä¸å­˜åœ¨")

        # æ­¥é©Ÿ 3: å»ºç«‹æ–°è³‡æ–™åº«
        print("\nğŸ“‹ æ­¥é©Ÿ 3: å»ºç«‹æ–°è³‡æ–™åº«...")
        if not create_database(admin_params, TARGET_DB_NAME):
            print("âŒ ç„¡æ³•å»ºç«‹è³‡æ–™åº«ï¼Œçµ‚æ­¢ç¨‹åº")
            return

        # æ­¥é©Ÿ 4: é€£æ¥åˆ°æ–°å»ºç«‹çš„è³‡æ–™åº«
        print("\nğŸ“‹ æ­¥é©Ÿ 4: é€£æ¥åˆ°æ–°è³‡æ–™åº«...")
        target_params = parse_database_url(DATABASE_URL)
        target_params['database'] = TARGET_DB_NAME
        conn = psycopg2.connect(**target_params)
        print("âœ… è³‡æ–™åº«é€£ç·šæˆåŠŸ\n")

        # æ­¥é©Ÿ 5: åŸ·è¡Œ schema.sql å»ºç«‹è¡¨æ ¼
        print("ğŸ“‹ æ­¥é©Ÿ 5: å»ºç«‹è³‡æ–™è¡¨çµæ§‹...")
        if not execute_sql_file(conn, SCHEMA_SQL_PATH):
            print("âŒ ç„¡æ³•å»ºç«‹è³‡æ–™è¡¨çµæ§‹ï¼Œçµ‚æ­¢ç¨‹åº")
            conn.close()
            return

        # æ­¥é©Ÿ 6: åŒ¯å…¥ CSV è³‡æ–™
        print("\nğŸ“‹ æ­¥é©Ÿ 6: åŒ¯å…¥ CSV è³‡æ–™...")
        import_order = [
            "member",
            "category",
            "staff",
            "pick_up_place",
            "item",
            "item_pick",
            "item_verification",
            "reservation",
            "reservation_detail",
            "contribution",
            "category_ban",
            "report",
            "loan",
            "loan_event",
            "review"
        ]

        success_count = 0
        for table_name in import_order:
            if table_name in TABLE_MAPPINGS:
                if import_table(conn, table_name, TABLE_MAPPINGS[table_name]):
                    success_count += 1

        print(f"\nâœ¨ CSV åŒ¯å…¥å®Œæˆï¼æˆåŠŸåŒ¯å…¥ {success_count}/{len(import_order)} å€‹è¡¨æ ¼")

        # æ­¥é©Ÿ 7: åŸ·è¡Œ setnextval.sql èª¿æ•´åºåˆ—
        print("\nğŸ“‹ æ­¥é©Ÿ 7: èª¿æ•´åºåˆ— (auto increment)...")
        if not execute_sql_file(conn, SETNEXTVAL_SQL_PATH):
            print("âš ï¸  èª¿æ•´åºåˆ—æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œä½†è³‡æ–™å·²åŒ¯å…¥")
        else:
            print("âœ… åºåˆ—èª¿æ•´å®Œæˆ")

        # æ­¥é©Ÿ 8: åŸ·è¡Œ setindex.sql å»ºç«‹ç´¢å¼•
        print("\nğŸ“‹ æ­¥é©Ÿ 8: å»ºç«‹è³‡æ–™åº«ç´¢å¼•...")
        if not execute_sql_file(conn, SETINDEX_SQL_PATH):
            print("âš ï¸  å»ºç«‹ç´¢å¼•æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼Œä½†è³‡æ–™å·²åŒ¯å…¥")
        else:
            print("âœ… ç´¢å¼•å»ºç«‹å®Œæˆ")

        # é—œé–‰é€£ç·š
        conn.close()

        print(f"\nğŸ‰ æ‰€æœ‰æ­¥é©Ÿå®Œæˆï¼è³‡æ–™åº« {TARGET_DB_NAME} å·²æº–å‚™å°±ç·’")

    except (psycopg2.Error, IOError, ValueError) as e:
        print(f"âŒ éŒ¯èª¤: {str(e)}")
        import traceback
        traceback.print_exc()


if __name__ == "__main__":
    main()
